"""
Wrapper utilities around the upstream python-scripture-scraper script.
"""

from __future__ import annotations

import shutil
from dataclasses import dataclass
from pathlib import Path
from textwrap import dedent
from typing import Iterable, List

import subprocess

DEFAULT_SCRAPER_ROOT = Path("external/python-scripture-scraper")
DEFAULT_OUTPUT_ROOT = Path("data/raw")


@dataclass(slots=True)
class ScrapeConfig:
    """Configuration knobs for running the upstream scraper.

    Example:
        >>> cfg = ScrapeConfig()
        >>> cfg.use_test_data
        True

    Set ``skip_existing_chapters`` to reuse chapter JSON already written under
    ``external/python-scripture-scraper/_output``.
    """

    use_test_data: bool = False
    pause_seconds: float = 0.6
    default_lang: str = "en"
    include_copyrighted: bool = True
    skip_existing_chapters: bool = True


def _config_text(cfg: ScrapeConfig) -> str:
    """Render the upstream config.py content for the given settings."""

    return (
        dedent(
            f"""
        # Auto-generated by scriptures.scraper
        DEFAULT_LANG = '{cfg.default_lang}'
        SCRAPE_FULL_CONTENT = True
        SCRAPE_METADATA_FOR_ALL_LANGUAGES = False
        SECONDS_TO_PAUSE_BETWEEN_REQUESTS = {cfg.pause_seconds}
        JSON_INDENT = 2
        USE_TEST_DATA = {str(cfg.use_test_data)}
        SKIP_EXISTING_CHAPTERS = {str(cfg.skip_existing_chapters)}
        if SCRAPE_FULL_CONTENT:
          OUTPUT_AS_JSON = True
          OUTPUT_AS_HTML = True
          OUTPUT_AS_MD = False
          OUTPUT_AS_TXT = False
          OUTPUT_AS_CSV = False
          OUTPUT_AS_TSV = False
          OUTPUT_AS_SQL_MYSQL = False
          OUTPUT_AS_SQL_SQLITE = False
          SPLIT_JSON_BY_CHAPTER = True
          MINIFY_JSON = False
          BASIC_HTML = False
          INCLUDE_IMAGES = True
          INCLUDE_COPYRIGHTED_CONTENT = {str(cfg.include_copyrighted)}
          INCLUDE_MEDIA_INFO = False
          ADD_CSS_STYLESHEET = False
        """
        ).strip()
        + "\n"
    )


def write_config(cfg: ScrapeConfig, scraper_root: Path = DEFAULT_SCRAPER_ROOT) -> Path:
    """Rewrite the upstream config.py to match our desired settings."""

    target = scraper_root / "resources" / "config.py"
    target.write_text(_config_text(cfg), encoding="utf-8")
    return target


def _copy_json_output(scraper_root: Path, dest_root: Path) -> Path:
    """Copy en-json output into a stable data directory."""

    src = scraper_root / "_output" / "en-json"
    dest_root.mkdir(parents=True, exist_ok=True)
    if dest_root.exists():
        shutil.rmtree(dest_root)
        dest_root.mkdir(parents=True, exist_ok=True)
    shutil.copytree(src, dest_root, dirs_exist_ok=True)
    return dest_root


def run_scraper(
    cfg: ScrapeConfig,
    scraper_root: Path = DEFAULT_SCRAPER_ROOT,
    dest_root: Path = DEFAULT_OUTPUT_ROOT,
) -> Path:
    """Execute the upstream scraper and copy JSON output.

    Returns:
        Path to the copied JSON directory.

    Example:
        >>> run_scraper(ScrapeConfig())  # doctest: +SKIP
    """

    write_config(cfg, scraper_root=scraper_root)
    subprocess.run(
        ["uv", "run", "python", "scrape.py"],
        check=True,
        cwd=scraper_root,
    )
    return _copy_json_output(scraper_root, dest_root)


def iter_chapter_paths(root: Path) -> List[Path]:
    """List all chapter JSON files beneath ``root``."""

    return sorted(root.glob("**/*.json"))
