{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2ecaec56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "verse                42513\n",
            "study-paragraph      1907\n",
            "chapter-title        1584\n",
            "study-footnotes      1579\n",
            "paragraph            244\n",
            "section-title        166\n",
            "book-title           92\n",
            "book-subtitle        10\n",
            "chapter-subtitle     8\n",
            "image                3\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import Counter as CounterType, Mapping\n",
        "\n",
        "\n",
        "def collect_paragraph_types(root: Path) -> CounterType[str]:\n",
        "    \"\"\"Return counts of every paragraph 'type' in scraper JSON files under root.\n",
        "\n",
        "    Args:\n",
        "        root: Directory that contains the scraped en-json tree.\n",
        "    Returns:\n",
        "        Counter mapping paragraph type -> occurrence count.\n",
        "    Example:\n",
        "        >>> collect_paragraph_types(Path(\"external/python-scripture-scraper/_output/en-json\"))  # doctest: +SKIP\n",
        "    \"\"\"\n",
        "    counts: CounterType[str] = Counter()\n",
        "    for path in root.rglob(\"*.json\"):\n",
        "        data: Mapping = json.loads(path.read_text())\n",
        "        for p in data.get(\"paragraphs\", []):\n",
        "            ptype = p.get(\"type\", \"<missing>\")\n",
        "            counts[ptype] += 1\n",
        "    return counts\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base = Path(\"external/python-scripture-scraper/_output/en-json\")\n",
        "    for ptype, count in collect_paragraph_types(base).most_common():\n",
        "        print(f\"{ptype:20s} {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e06bf719",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "ALLOWED_VERSE_TYPES = {\"verse\", \"study-footnotes\"}\n",
        "\n",
        "\n",
        "def chapters_with_intermingled(root: Path) -> Iterable[tuple[Path, list[str]]]:\n",
        "    \"\"\"Yield chapters whose non-verse paragraphs still lack paragraphCategory.\"\"\"\n",
        "\n",
        "    for path in root.rglob(\"*.json\"):\n",
        "        data = json.loads(path.read_text())\n",
        "        paragraphs = data.get(\"paragraphs\", [])\n",
        "        if not isinstance(paragraphs, list):\n",
        "            continue\n",
        "        types = [p.get(\"type\", \"<missing>\") for p in paragraphs]\n",
        "        verse_idxs = [i for i, t in enumerate(types) if t in ALLOWED_VERSE_TYPES]\n",
        "        if len(verse_idxs) < 2:\n",
        "            continue\n",
        "        span_start, span_end = min(verse_idxs), max(verse_idxs)\n",
        "        span_paragraphs = paragraphs[span_start : span_end + 1]\n",
        "        missing_categories = sorted(\n",
        "            {\n",
        "                p.get(\"type\", \"<missing>\")\n",
        "                for p in span_paragraphs\n",
        "                if (p.get(\"type\") not in ALLOWED_VERSE_TYPES)\n",
        "                and not p.get(\"paragraphCategory\")\n",
        "            }\n",
        "        )\n",
        "        if missing_categories:\n",
        "            yield path, missing_categories\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base = Path(\"external/python-scripture-scraper/_output/en-json\")\n",
        "    for chap_path, offenders in chapters_with_intermingled(base):\n",
        "        rel = chap_path.relative_to(base)\n",
        "        print(f\"{rel}: {', '.join(offenders)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b848c23e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "para_type,book,chapters,n_chapters\n",
            "chapter-subtitle,moses,1 2 3 4 5 6 7 8,8\n",
            "image,abraham,fac-1 fac-2 fac-3,3\n",
            "paragraph,1-nephi,1,1\n",
            "paragraph,2-nephi,1,1\n",
            "paragraph,3-nephi,1 11,2\n",
            "paragraph,4-nephi,1,1\n",
            "paragraph,abraham,fac-1 fac-2 fac-3,3\n",
            "paragraph,alma,1 5 7 9 17 21 36 38 39 45,10\n",
            "paragraph,helaman,1 7 13,3\n",
            "paragraph,jacob,1,1\n",
            "paragraph,moroni,9,1\n",
            "paragraph,mosiah,9 23,2\n",
            "paragraph,psalms,3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 34 35 36 37 38 39 40 41 42 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 92 98 100 101 102 103 108 109 110 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 138 139 140 141 142 143 144 145,116\n",
            "section-title,jst-1-chronicles,21,1\n",
            "section-title,jst-1-corinthians,7 15,2\n",
            "section-title,jst-1-john,2 3 4,3\n",
            "section-title,jst-1-peter,3,1\n",
            "section-title,jst-1-samuel,16,1\n",
            "section-title,jst-1-thessalonians,4,1\n",
            "section-title,jst-1-timothy,2 3 6,3\n",
            "section-title,jst-2-chronicles,18,1\n",
            "section-title,jst-2-corinthians,5,1\n",
            "section-title,jst-2-peter,3,1\n",
            "section-title,jst-2-samuel,12,1\n",
            "section-title,jst-2-thessalonians,2,1\n",
            "section-title,jst-acts,9 22,2\n",
            "section-title,jst-amos,7,1\n",
            "section-title,jst-colossians,2,1\n",
            "section-title,jst-deuteronomy,10,1\n",
            "section-title,jst-ephesians,4,1\n",
            "section-title,jst-exodus,18 22 32 33,4\n",
            "section-title,jst-galatians,3,1\n",
            "section-title,jst-genesis,14 15 19 21 48 50 1-8,7\n",
            "section-title,jst-hebrews,1 4 6,3\n",
            "section-title,jst-isaiah,29 42,2\n",
            "section-title,jst-james,1,1\n",
            "section-title,jst-jeremiah,26,1\n",
            "section-title,jst-john,6 13 14,3\n",
            "section-title,jst-luke,1 2 6 9 11 14 16 18 23 24,10\n",
            "section-title,jst-mark,2 3 7 12 16,5\n",
            "section-title,jst-matthew,5 9 11 12 13 16 17 18 19 23 26 27,12\n",
            "section-title,jst-psalms,11 14 24 109,4\n",
            "section-title,jst-revelation,1 5 12 19,4\n",
            "section-title,jst-romans,3 7 13,3\n",
            "section-title,official-declarations,1,1\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "from typing import DefaultDict, Dict, Iterable, Set, Tuple\n",
        "\n",
        "ALLOWED_TYPES: Set[str] = {\n",
        "    \"verse\",\n",
        "    \"chapter-title\",\n",
        "    \"study-paragraph\",\n",
        "    \"study-footnotes\",\n",
        "    \"book-title\",\n",
        "    \"book-subtitle\",\n",
        "}\n",
        "\n",
        "\n",
        "def paragraph_missing_category(paragraph: dict) -> bool:\n",
        "    \"\"\"Return True when the paragraph type is unclassified and should be flagged.\"\"\"\n",
        "\n",
        "    if not paragraph:\n",
        "        return False\n",
        "    ptype = paragraph.get(\"type\", \"<missing>\")\n",
        "    if ptype in ALLOWED_TYPES:\n",
        "        return False\n",
        "    return not paragraph.get(\"paragraphCategory\")\n",
        "\n",
        "\n",
        "def chapter_number(data: Dict, path: Path) -> str:\n",
        "    \"\"\"Return chapter number from JSON, falling back to filename stem.\"\"\"\n",
        "\n",
        "    number = data.get(\"number\")\n",
        "    if number:\n",
        "        return str(number)\n",
        "    stem = path.stem\n",
        "    return stem.rsplit(\"-\", 1)[-1]\n",
        "\n",
        "\n",
        "def find_outlier_chapters(root: Path) -> DefaultDict[Tuple[str, str], Set[str]]:\n",
        "    \"\"\"Map (para_type, book) -> set of chapter numbers lacking paragraphCategory.\"\"\"\n",
        "\n",
        "    outliers: DefaultDict[Tuple[str, str], Set[str]] = defaultdict(set)\n",
        "    for path in root.rglob(\"*.json\"):\n",
        "        data = json.loads(path.read_text())\n",
        "        book = path.parent.name\n",
        "        chap = chapter_number(data, path)\n",
        "        paragraphs = data.get(\"paragraphs\")\n",
        "        if not isinstance(paragraphs, list):\n",
        "            continue\n",
        "        missing_types = {\n",
        "            p.get(\"type\", \"<missing>\")\n",
        "            for p in paragraphs\n",
        "            if paragraph_missing_category(p)\n",
        "        }\n",
        "        for t in missing_types:\n",
        "            outliers[(t, book)].add(chap)\n",
        "    return outliers\n",
        "\n",
        "\n",
        "def print_pivot(outliers: DefaultDict[Tuple[str, str], Set[str]]) -> None:\n",
        "    \"\"\"Print pivot table of remaining missing categories.\"\"\"\n",
        "\n",
        "    print(\"para_type,book,chapters,n_chapters\")\n",
        "    for ptype, book in sorted(outliers):\n",
        "        chapters = sorted(outliers[(ptype, book)], key=lambda x: (len(x), x))\n",
        "        chap_str = \" \".join(chapters)\n",
        "        print(f\"{ptype},{book},{chap_str},{len(chapters)}\")\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"Scan scraper output and report paragraph types still needing categories.\"\"\"\n",
        "\n",
        "    root = Path(\"external/python-scripture-scraper/_output/en-json\")\n",
        "    pivot = find_outlier_chapters(root)\n",
        "    print_pivot(pivot)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9614e6fe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tags (top 11):\n",
            "  a: 117393\n",
            "  li: 72807\n",
            "  sup: 47192\n",
            "  span: 28505\n",
            "  ul: 25615\n",
            "  small: 24189\n",
            "  em: 1788\n",
            "  strong: 143\n",
            "  i: 88\n",
            "  br: 39\n",
            "  img: 3\n",
            "\n",
            "Classes (top 6):\n",
            "  scripture-ref: 70193\n",
            "  footnote-link: 47200\n",
            "  marker: 47192\n",
            "  clarity-word: 21579\n",
            "  small-caps: 6895\n",
            "  uppercase: 31\n",
            "\n",
            "Tag/Class pairs (top 6):\n",
            "  ('a', 'scripture-ref'): 70193\n",
            "  ('a', 'footnote-link'): 47200\n",
            "  ('sup', 'marker'): 47192\n",
            "  ('span', 'clarity-word'): 21579\n",
            "  ('span', 'small-caps'): 6895\n",
            "  ('span', 'uppercase'): 31\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Utilities to tally HTML tags and classes in scraped chapter content.\n",
        "\n",
        "Run directly to print counts:\n",
        "    uv run python scripts/html_inventory.py --root external/python-scripture-scraper/_output/en-json\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Iterable, Tuple\n",
        "\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "@dataclass(slots=True)\n",
        "class HtmlInventory:\n",
        "    \"\"\"Aggregated counts of tags and classes.\n",
        "\n",
        "    Attributes:\n",
        "        tag_counts: occurrences keyed by tag name.\n",
        "        class_counts: occurrences keyed by individual class name.\n",
        "        tag_class_counts: occurrences keyed by (tag, class) pairs.\n",
        "\n",
        "    Example:\n",
        "        >>> inv = HtmlInventory(Counter({\"p\": 2}), Counter({\"verse\": 1}), Counter({(\"p\", \"verse\"): 1}))\n",
        "        >>> inv.tag_counts[\"p\"]\n",
        "        2\n",
        "    \"\"\"\n",
        "\n",
        "    tag_counts: Counter[str]\n",
        "    class_counts: Counter[str]\n",
        "    tag_class_counts: Counter[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def collect_html_inventory(root: Path) -> HtmlInventory:\n",
        "    \"\"\"Walk JSON chapter files under ``root`` and tally HTML tags/classes.\n",
        "\n",
        "    Args:\n",
        "        root: Directory containing chapter JSON files (split by chapter).\n",
        "\n",
        "    Returns:\n",
        "        HtmlInventory with counters populated from all ``contentHtml`` values.\n",
        "    \"\"\"\n",
        "\n",
        "    assert root.exists(), f\"Missing data directory: {root}\"\n",
        "    tag_counts: Counter[str] = Counter()\n",
        "    class_counts: Counter[str] = Counter()\n",
        "    tag_class_counts: Counter[Tuple[str, str]] = Counter()\n",
        "\n",
        "    for path in root.rglob(\"*.json\"):\n",
        "        with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
        "            data = json.load(fh)\n",
        "        paragraphs: Iterable[dict] = (\n",
        "            data.get(\"paragraphs\", []) if isinstance(data, dict) else []\n",
        "        )\n",
        "        for para in paragraphs:\n",
        "            html = para.get(\"contentHtml\")\n",
        "            if not html:\n",
        "                continue\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            for el in soup.find_all(True):\n",
        "                tag_counts[el.name] += 1\n",
        "                for cls in el.get(\"class\", []):\n",
        "                    class_counts[cls] += 1\n",
        "                    tag_class_counts[(el.name, cls)] += 1\n",
        "\n",
        "    return HtmlInventory(\n",
        "        tag_counts=tag_counts,\n",
        "        class_counts=class_counts,\n",
        "        tag_class_counts=tag_class_counts,\n",
        "    )\n",
        "\n",
        "\n",
        "def _top(counter: Counter, limit: int = 25):\n",
        "    \"\"\"Return the most common items up to ``limit`` entries.\"\"\"\n",
        "\n",
        "    return counter.most_common(limit)\n",
        "\n",
        "\n",
        "def main(\n",
        "    root: str = \"external/python-scripture-scraper/_output/en-json\",\n",
        "    limit: int = 25,\n",
        "    pair_limit: int = 100,\n",
        ") -> None:\n",
        "    \"\"\"Print tag/class frequency tables for the scraped chapters.\n",
        "\n",
        "    Args:\n",
        "        root: Base directory containing chapter JSON files.\n",
        "        limit: Number of tag and class rows to display.\n",
        "        pair_limit: Number of tag/class pair rows to display.\n",
        "    \"\"\"\n",
        "\n",
        "    inventory = collect_html_inventory(Path(root))\n",
        "\n",
        "    def fmt(title: str, items):\n",
        "        print(f\"\\n{title} (top {len(items)}):\")\n",
        "        for key, count in items:\n",
        "            print(f\"  {key}: {count}\")\n",
        "\n",
        "    fmt(\"Tags\", _top(inventory.tag_counts, limit))\n",
        "    fmt(\"Classes\", _top(inventory.class_counts, limit))\n",
        "    fmt(\"Tag/Class pairs\", _top(inventory.tag_class_counts, pair_limit))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f08e502",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import random\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "from typing import DefaultDict, List, Tuple\n",
        "\n",
        "\n",
        "def summarize_word_seq(text: str, limit: int = 10) -> str:\n",
        "    tokens = re.findall(r\"\\S+\", text)\n",
        "    return \" \".join(tokens[:limit]) or \"(empty)\"\n",
        "\n",
        "\n",
        "def collect_paragraph_metadata(root: Path) -> Tuple[Counter[str], DefaultDict[str, List[Tuple[str, str, str]]]]:\n",
        "    counter: Counter[str] = Counter()\n",
        "    candidates: DefaultDict[str, List[Tuple[str, str, str]]] = defaultdict(list)\n",
        "\n",
        "    for json_path in root.rglob(\"*.json\"):\n",
        "        data = json.loads(json_path.read_text())\n",
        "        paragraphs = data.get(\"paragraphs\", [])\n",
        "        if not isinstance(paragraphs, list):\n",
        "            continue\n",
        "        rel_path = json_path.relative_to(root)\n",
        "        abbreviation = data.get(\"abbrev\") or data.get(\"name\")\n",
        "        ref_label = abbreviation or str(rel_path)\n",
        "        for paragraph in paragraphs:\n",
        "            category = paragraph.get(\"paragraphCategory\") or \"<missing>\"\n",
        "            counter[category] += 1\n",
        "            candidates[category].append((ref_label, paragraph.get(\"id\", \"<no-id>\"), summarize_word_seq(paragraph.get(\"content\", \"\"))))\n",
        "    return counter, candidates\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    root = Path(\"external/python-scripture-scraper/_output/en-json\")\n",
        "    counts, candidates = collect_paragraph_metadata(root)\n",
        "    for category, count in counts.most_common():\n",
        "        print(f\"{category}: {count}\")\n",
        "        sample = candidates.get(category, [])\n",
        "        if sample:\n",
        "            for ref_label, para_id, excerpt in random.sample(sample, min(5, len(sample))):\n",
        "                print(f\"  {ref_label}#{para_id}: {excerpt}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Scriptures",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}